# CS663 - Artificial Intelligence Lab  
### Lab Report 2 (Assignments 5â€“8)  
Submitted by **Team37**  
Autumn 2025â€“26, IIIT Vadodara  
Instructor: **Pratik Shah**

---

## About This Repository  
This repository contains the implementations and analysis for **Assignments 5 to 8** of the CS663 Artificial Intelligence Lab.  
The work spans probabilistic graphical models, neural associative memory, reinforcement learning, and dynamic programming.  

---

# **Assignment 5 â€” Bayesian Networks & Gaussian HMMs**

This assignment focuses on probabilistic reasoning using Bayesian Networks and Hidden Markov Models.

### **Key Components**
- Data preprocessing from real grade and financial datasets  
- BN structure learning and CPT estimation  
- Naive Bayes classification experiment (20 random trials)  
- HMM fitting, decoding, and regime interpretation  



# **Assignment 6 â€” Hopfield Networks & Combinatorial Optimization**

This assignment explores Hopfield Networks as associative memory systems and optimization engines.

### **Key Components**
- Weight matrix construction using Hebbian learning  
- Checking stability and recall accuracy  
- Energy function formulation  
- Constraint encoding for Rook and TSP problems  

# **Assignment 7 â€” MENACE & Multi-Armed Bandits**

This assignment introduces reinforcement learning concepts through MENACE and bandit algorithms.

### **Key Components**
- Custom Python implementation of MENACE  
- Experiments with binaryBanditA and binaryBanditB  
- Non-stationary bandit environment (`bandit_nonstat`)  
- Agent performance comparison with/without tracking  
- Reward curves and convergence behavior

# **Assignment 8 â€” Markov Decision Processes & Dynamic Programming**

This assignment covers stochastic sequential decision-making and policy optimization.

## **Part A â€” Gridworld MDP**
- 4Ã—3 stochastic environment  
- Transition slip model:  
  - 0.8 intended direction  
  - 0.1 left/right  
- Terminal states with +1 and âˆ’1 reward  
- Value iteration for:  
  - r(s) = âˆ’2  
  - r(s) = 0.1  
  - r(s) = 0.02  
  - r(s) = 1  
- Extraction of optimal value function and policy

## **Part B â€” Gbike Bicycle Rental Problem**
A modified version of Jackâ€™s Car Rental from Sutton & Barto.


### **Outputs**
- Optimal value function  
- Optimal movement policy  
- Policy heatmap  
- Value-function heatmap  


## ðŸ“‚ Repository Structure

